1. 存储高可用方案的本质都是通过将数据复制到多个存储设备，通过数据冗余的方式来实现高可用，其复杂性主要体现在如何应对复制延迟和中断导致的数据不一致的问题。因此，对任何一个高可用存储方案，我们需要从一下几个反面取思考和分析
    * 数据如何复制
    * 各个节点的职责是什么
    * 如何应对复制延迟
    * 如何应对复制中断
#### 双机架构
1. 主备
![](img/%E4%B8%BB%E5%A4%87%E5%A4%8D%E5%88%B6.jpeg)  
    * 优点就是简单
        - 对客户端来说无需感知备机的存在
        - 对主机跟备机来说，双方只需要复制数据即可，无需进行状态判断及主备切换操作
    * 缺点：备机仅仅只为备份，没有提供读写能力，硬件成本上又浪费
2. 主从
![](img/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6.jpeg)
    * 跟主备类似，不过从机提供了读能力，在硬件成本上没有浪费
    * 客户端需要感知从机的存在，并且将不同的操作发送给相应的机器
    * 如果主从延迟较大会出现数据不一致的情况
    * 常用语读多写少的场景
3. 主备/主从切换
![](img/%E5%8F%8C%E6%9C%BA%E5%88%87%E6%8D%A2.jpeg)
    * 设计关键
        - 主备跟主从存在两个共性的问题
            + 主机故障后无法进行写操作
            + 如果主机无法恢复需要人工指定新的主机角色
        - 关键点
            + 状态传递的渠道
                - 互相连接还是第三方仲裁
            + 状态检测的内容
                - 机器是否掉电、进程是否存在、响应是否缓慢等
            + 切换时机
                - 什么情况下备机应该升级为主机？掉电后？进程不存在？响应超过3s，3分钟内重启3次？
            + 切换策略
                - 原来的主机故障恢复后再次切换，确保原来的主机继续做主机，还是原来的主机故障恢复后，自动成为备机
            + 自动程度
                - 全自动/半自动（需要人工确认）
    * 常见架构
        - 互联式
        ![](img/%E4%BA%92%E8%81%94%E5%BC%8F.jpeg)
            + 缺点是状态传递的通道本身有故障，那么备机也会认为主机故障从而将自己升级为主机，而此时主机没有故障，最终可能出现两个主机
            + 
        - 中介式
        ![](img/%E4%B8%AD%E4%BB%8B%E5%BC%8F.jpeg)
            + 连接管理简单，主备机间无需进行状态同步，只需与中介建立连接即可
            + 状态决策简单
            + 中介机的高可用问题，常用方案是使用ZK
        - 模拟式
        ![](img/%E6%A8%A1%E6%8B%9F%E5%BC%8F.jpeg)
            + 主备机间不传递任何状态数据，而是备机模拟成一个客户端，向主机发起模拟的读写操作，根据读写操作的响应情况来判断主机的状态。
            + 优点是实现简单，不需要状态传递的通道建立与管理工作
            + 缺点是读写操作的响应信息比较简单，决策可能出现偏差
4. 主主
![](img/%E7%8C%AA%E7%8C%AA.jpeg)
    + 两台都是主机，不存在切换的概念
    + 客户端不需要区分服务器的角色
    + 需要保证数据能够双向复制
    + 一般适用临时性、可丢失、可覆盖的数据场景

#### 集群和分区
1. 数据集群
    * 主备、主从、主主架构本质上都有一个隐含的假设：主机能够存储所有数据，但主机本身的存储和处理能力肯定是有极限的。
    * 数据集中集群
    ![](img/%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E9%9B%86%E7%BE%A4.jpeg)
    * 数据分散机群
        - 多个服务器组成一个集群，每台服务器都会负责储存一部分数据；同时为了提升硬件利用率，每台服务器又会备份一部分数据。
        - 复杂点在于如何将数据分配到不同的服务器上
            + 算法需要保证服务器上的数据分区基本是均衡的
            + 当出现部分服务器故障时，算法需要将故障服务器的数据分配给其他服务器
            + 集群容量不够时，扩容新服务器后，算法能够将部分数据分区迁移到新服务器，并保证扩容后所有服务器的均衡性
2. 数据分区
    * 将数据按照一定的规则进行分区，不同分区分布在不同的地理位置上，每个分区存储一部分数据，通过这种方式来规避地理级别的故障所造成的巨大影响。采用数据分区架构后，即使某个地区发生严重的自然灾害或事故，受影响的也只是一部分数据，而不是全部数据都不可用；当故障恢复后其他地区备份的数据也可以帮助故障地区快速恢复业务。
    * 数据量，数据量的大小直接决定了分区的规则复杂度。
    * 分区规则，地理位置有缘有近，因此可以得到不同分区规则，包括洲际分区，国家分区，城市分区
    * 复制规则
        - 集中式，存在一个总的备份中心，所有分区数据都备份到备份中心
        ![](img/%E9%9B%86%E4%B8%AD%E5%BC%8F%E5%A4%87%E4%BB%BD%E4%B8%AD%E5%BF%83.jpeg)
        - 互备式,每个分区备份另外一个分区的数据
        ![](img/%E4%BA%92%E5%A4%87%E5%BC%8F.jpeg)
            + 设计比较复杂，各个分区除了承担存储功能还要承担备份功能，各分区间相互关联和影响
            + 扩展比较麻烦，新增或删除数据中心是需要迁移部分数据
            + 成本低，直接利用现有设备
        - 独立式，每个分区都有自己独立的备份中心
        ![](img/%E7%8B%AC%E7%AB%8B%E5%BC%8F.jpeg)
            + 设计简单，各分区间互不影响
            + 扩展容易，新建的分区只需要搭建自己的备份中心即可
            + 成本高